{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIDtZX2fbAxBm/ZW+rlSid",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wycology/Bayes_Regression/blob/main/dl_tea.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='green'><b> SATELLITE DATA FOR AGRICULTURAL ECONOMISTS</b></font>\n",
        "\n",
        "\n",
        "<font color='blue'><b>THEORY AND PRACTICE</b></font>\n",
        "\n",
        "**Mapping tea plantations in Central Kenya: _Deep Learning Approach_**\n",
        "\n",
        "\n",
        "*David Wuepper, Lisa Biber-Freudenberger, Hadi, Wyclife Agumba Oluoch*\n",
        "\n",
        "[Land Economics Group](https://www.ilr1.uni-bonn.de/en/research/research-groups/land-economics), University of Bonn, Bonn, Germany\n",
        "\n",
        "---\n",
        "\n",
        "# **Background**\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TkemFw3E0WAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we introduce basics of using deep learning approach to segment tea fields with a practical example at the foot of Mount Kenya. We obtained high resolution Sentinel-2 satellite image from [Google Earth Engine](https://code.earthengine.google.com/60cf3e783458009bd8378eaded30f5c7). On the other hand, we obtained labels by manually digitizing tea plantations within QGIS using Google Satellite Hybrid basemap. The labels cover a small portion of the downloaded Satellite image so that we can train the model and use it to segment tea fields elsewhere.\n",
        "We used torchgeo for this modeling task due to the following reasons:\n",
        "1. It is simple to use, eliminating a lot of issues such as georeferencing, chipping, label creation.\n",
        "2. It also maximally obtain training samples from the region of interest."
      ],
      "metadata": {
        "id": "UoK9t_IT0d7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries\n",
        "---\n",
        "Since [torchgeo](https://torchgeo.readthedocs.io/en/latest/) is not natively installed in colab, we will have to install it. We will also install [torchseg](https://pypi.org/project/torchseg/0.0.1a1/) to help with the segmentation work. Other supporting libraries will just be imported as they are already pre-installed in colab."
      ],
      "metadata": {
        "id": "_jd9jmSt0iue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries not already available in colab\n",
        "!pip install torchgeo\n",
        "!pip install torchseg"
      ],
      "metadata": {
        "id": "ikM6_Uno0cQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import json\n",
        "import torch\n",
        "import rasterio\n",
        "import torchseg\n",
        "import torchgeo\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchgeo.transforms import AppendNDVI\n",
        "from rasterio.transform import from_bounds\n",
        "from torchgeo.samplers import RandomGeoSampler, GridGeoSampler\n",
        "from torchgeo.datasets import VectorDataset, RasterDataset, stack_samples"
      ],
      "metadata": {
        "id": "EPvUqX9Z0mny"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installing the libraries, we make an important step of confirming the working directory. This is important since both our image and gpkg will be read from this location so we need to be sure of the path. We can use `pwd` function to print it."
      ],
      "metadata": {
        "id": "jcnZD7xt0ty0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd # Confirm the working directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Dg5Z4M1-0rN8",
        "outputId": "c5249054-2ab6-4133-8781-0fdcd780fae7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Dataset\n",
        "---\n",
        "With `torchgeo`, we do not have to pre-chip the satellite image into small chips of say 256 by 256 pixels. This it achieves on the fly. However, we need to tell it the path to where our satellite image is. In fact, we can have several large images here. For now, it is the only _.tif_ image our working directory, wo we define the class as follows:"
      ],
      "metadata": {
        "id": "hRrZylsA0zhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the GeoTiff dataset class\n",
        "\n",
        "class GeoTiffDataset(RasterDataset):\n",
        "  filename_glob = \"*.tif\"\n",
        "raster_data = GeoTiffDataset(paths = \"/content\")"
      ],
      "metadata": {
        "id": "VBq6ifSE0rKh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In that case, raster_data is a blueprint of the satellite image we have in the directory. Next, we do the same for the label data. This label data is a _.gpkg_ file which has a column stating the identity of the each polygon as either tea or not tea. In other words, the class column. Here, we call the class column as `tea_no_tea`. This is very important as it is what the library uses to create a label binary layer under the hood to intersect with the satellite image. We achieve this as follows:"
      ],
      "metadata": {
        "id": "gASRx2B-05QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the label (here vector but can also be mask raster) dataset class.\n",
        "# Remember to include label name. Never forget this!\n",
        "\n",
        "class LabelDataset(VectorDataset):\n",
        "  filename_glob = \"*.gpkg\"\n",
        "label_data = LabelDataset(paths = \"/content\", label_name = \"tea_no_tea\")"
      ],
      "metadata": {
        "id": "NGv90kvA0rHW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining `raster_data` and `label_data`\n",
        "___\n",
        "Now that we have blueprints of both raster_data and label_data, the next step is to intersect the two. This will behave like _cropping_ or _clipping_ the raster to the extent of the label_data. This is the reason why we are not worried that the raster extent is bigger than the label extent. Chips for training the model will only be ontained from where the two datasets intersect/overlap. Regions outside the label_data will not be sampled. As simple as it can get, we achieve this intersection using an _&_ operator."
      ],
      "metadata": {
        "id": "GXrnN5391E1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the intersection of the raster and vector/label datasets\n",
        "\n",
        "training_data = raster_data & label_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_skUtBwb0rEO",
        "outputId": "42a8f2f7-61d7-46a4-d5ee-6bd39b38dd10"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting LabelDataset res from (0.0001, 0.0001) to (10.0, 10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You notice the printout that **Converting LabelDataset res from (0.0001, 0.0001) to (10.0, 10.0)**. This tells us that our vector label data with polygons has now been converted to a binary raster under the hood with a pixel size similar to that of the satellite image we have - Sentinel-2. Something imortant to note also is that the pixels in both layers has been **aligned**."
      ],
      "metadata": {
        "id": "r5gzrFam1La-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create sampler\n",
        "---\n",
        "Since we did not pre-chip the satellite image, we need to provide a sampler which will do the task of obtaining small chips from the original image and pushing them to the model for training at a time. This is important since we cannot push the whole image to the model at once. In this case, we use [RandomGeoSampler](https://torchgeo.readthedocs.io/en/latest/api/samplers.html) which randomly picks unique but overlaping patches from the region of interest. This allows us to pick more patches from the study area than if we pre-chipped it. We achieve this as follows:"
      ],
      "metadata": {
        "id": "oFt4qcvc4dyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sampler that will execute the task of extracting samples\n",
        "\n",
        "sampler = RandomGeoSampler(dataset = training_data, size = 32, length = 1000)"
      ],
      "metadata": {
        "id": "7C7LvsPL0q9e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create data loader**\n",
        "---\n",
        "Once we have created a sampler that splits the big image to small patches within the region of interest, we do the next task of creating a data loader. This is what collected the slices or patches from the sampler in groups and pushes them to the model for training, validation, and testing as well as prediction tasks. This is attained as follows:"
      ],
      "metadata": {
        "id": "EKl0OOJ1509F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the dataloader. This is the function that will be serving the role of availing batches of extracted samples for model training\n",
        "dataloader = DataLoader(\n",
        "    dataset = training_data,\n",
        "    batch_size = 50,\n",
        "    sampler = sampler,\n",
        "    collate_fn = stack_samples\n",
        ")"
      ],
      "metadata": {
        "id": "1vSP5CIt1X5_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the loader\n",
        "\n",
        "Here we then check whether the dataloader is working as expected. That is, whether it is getting 50 small images and 50 labels from the sampler to take to the model."
      ],
      "metadata": {
        "id": "zAdX1YMt7HMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the dataloader to confirm that it is able to load the data\n",
        "\n",
        "for batch in dataloader:\n",
        "  image = batch[\"image\"][:, :11, :, :]\n",
        "  mask = image[:, -1, :, :]\n",
        "\n",
        "  print(f\"Image batch length: {len(image)}\")\n",
        "  print(f\"Mask batch length: {len(mask)}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvP_3oAi1X2u",
        "outputId": "a869e183-d419-43a2-9135-828c628470d6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch length: 50\n",
            "Mask batch length: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define the model**\n",
        "---\n",
        "\n",
        "Here we then remember our torchseg to give us a model/method to train our data which the sampler has sliced and the loader has gathered. torchseg has several models but we will use Unet which is a very common deep learning model, especially for segmentation tasks. In this setup, `encoder_name=\"resnet18\"` specifies the encoder, which is the feature extractor part. `encoder_weights=False` means we are not using pre-trained weights from imagenet, we will train it from scratch from our data. `in_channels=10` indicates that our Sentinel-2 image has 10 channels/bands. This is different from 3 channels of RGB. Lastly `classes=2` sets the number of output classes, which in our case is 2, tea and non-tea.\n"
      ],
      "metadata": {
        "id": "7Jq8G2S37hpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use Unet model from torchseg.\n",
        "# Which is pretrained so we do not need to build it from scratch\n",
        "model = torchseg.Unet(\n",
        "    encoder_name = \"resnet18\",\n",
        "    encoder_weights = False,\n",
        "    in_channels = 10,\n",
        "    classes = 2\n",
        ")"
      ],
      "metadata": {
        "id": "99g0hQ-h1Xzj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use cuda if available, otherwise cpu\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "EqMhnBGG1XwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = -1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "gVGGo31n1j0t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "metrics = {\"loss\": [], \"accuracy\": []}\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "yQK1Wzto1jw4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  total_correct = 0\n",
        "  total_pixels = 0\n",
        "\n",
        "  with tqdm(dataloader, desc = f\"Epoch {epoch + 1} / {num_epochs}\") as pbar:\n",
        "    for batch in pbar:\n",
        "      images = batch[\"image\"][:, :12, :, :].to(device)\n",
        "      masks = batch[\"mask\"].to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, masks.long())\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Calculate accuracy\n",
        "      preds = torch.argmax(outputs, dim = 1)\n",
        "      total_correct += (preds == masks).sum().item()\n",
        "      total_pixels += masks.numel()\n",
        "\n",
        "      pbar.set_postfix(loss = loss.item())\n",
        "\n",
        "  epoch_accuracy = total_correct / total_pixels * 100\n",
        "  metrics[\"loss\"].append(epoch_loss)\n",
        "  metrics[\"accuracy\"].append(epoch_accuracy)\n",
        "\n",
        "  print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "  # Save training metrics\n",
        "  with open(\"/content/training_metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics, f)\n",
        "\n",
        "  print(\"Training metrics saved to '/content/training_metrics.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l08G_R41jtA",
        "outputId": "088e48f8-8471-47aa-8f85-bd9834bd14dc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 / 10: 100%|██████████| 20/20 [01:55<00:00,  5.77s/it, loss=0.276]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 8.9851, Accuracy: 78.81%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 / 10: 100%|██████████| 20/20 [02:03<00:00,  6.16s/it, loss=0.193]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 4.5624, Accuracy: 90.61%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 / 10: 100%|██████████| 20/20 [02:01<00:00,  6.09s/it, loss=0.154]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 3.4800, Accuracy: 92.91%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 / 10: 100%|██████████| 20/20 [02:07<00:00,  6.36s/it, loss=0.132]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 2.8410, Accuracy: 94.30%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 / 10: 100%|██████████| 20/20 [02:08<00:00,  6.44s/it, loss=0.111]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 2.4096, Accuracy: 95.23%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 / 10: 100%|██████████| 20/20 [01:58<00:00,  5.94s/it, loss=0.105]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 2.0889, Accuracy: 95.92%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 / 10: 100%|██████████| 20/20 [02:03<00:00,  6.17s/it, loss=0.0866]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 1.8027, Accuracy: 96.55%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 / 10: 100%|██████████| 20/20 [02:04<00:00,  6.20s/it, loss=0.0705]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 1.5808, Accuracy: 97.10%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 / 10: 100%|██████████| 20/20 [01:59<00:00,  5.96s/it, loss=0.0616]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 1.4041, Accuracy: 97.51%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 / 10: 100%|██████████| 20/20 [02:03<00:00,  6.19s/it, loss=0.0586]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 1.2407, Accuracy: 97.87%\n",
            "Training metrics saved to '/content/training_metrics.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training metrics from JSON file\n",
        "with open(\"/content/training_metrics.json\", \"r\") as f:\n",
        "    metrics = json.load(f)\n",
        "\n",
        "# Extract loss and accuracy values\n",
        "loss_values = metrics[\"loss\"]\n",
        "accuracy_values = metrics[\"accuracy\"]\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "# Create the plots\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# Plot loss\n",
        "ax1.set_xlabel(\"Epochs\")\n",
        "ax1.set_ylabel(\"Loss\", color=\"tab:red\")\n",
        "ax1.plot(epochs, loss_values, label=\"Loss\", color=\"tab:red\")\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
        "\n",
        "# Create a second y-axis for accuracy\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel(\"Accuracy (%)\", color=\"tab:blue\")\n",
        "ax2.plot(epochs, accuracy_values, label=\"Accuracy\", color=\"tab:blue\")\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.title(\"Training Loss & Accuracy Over Epochs\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h13n9smM1q69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model.eval()\n",
        "\n",
        "# Path to your input raster\n",
        "raster_path = \"/content/s2_d.tif\"\n",
        "output_path = \"/content/predicted_output.tif\"\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Read the raster file\n",
        "with rasterio.open(raster_path) as src:\n",
        "    image = src.read()  # Shape: (bands, height, width)\n",
        "    transform = src.transform\n",
        "    crs = src.crs\n",
        "    profile = src.profile\n",
        "\n",
        "# Get original height and width\n",
        "orig_h, orig_w = image.shape[1], image.shape[2]\n",
        "\n",
        "# Compute padding needed\n",
        "pad_h = (32 - (orig_h % 32)) % 32  # Ensure divisibility by 32\n",
        "pad_w = (32 - (orig_w % 32)) % 32\n",
        "\n",
        "# Pad image (bottom, right)\n",
        "image_padded = np.pad(image, ((0, 0), (0, pad_h), (0, pad_w)), mode='reflect')\n",
        "\n",
        "# Convert to tensor and move to device\n",
        "image_tensor = torch.tensor(image_padded, dtype=torch.float32).unsqueeze(0).to(device)  # (1, bands, H, W)\n",
        "\n",
        "# Run the model\n",
        "with torch.no_grad():\n",
        "    output = model(image_tensor)\n",
        "\n",
        "# Convert predictions to class labels\n",
        "pred_mask_padded = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()  # (H, W)\n",
        "\n",
        "# Remove padding to match original shape\n",
        "pred_mask = pred_mask_padded[:orig_h, :orig_w]\n",
        "\n",
        "# Save the output raster\n",
        "profile.update(dtype=rasterio.uint8, count=1, height=orig_h, width=orig_w, nodata = 0)  # Update metadata\n",
        "\n",
        "with rasterio.open(output_path, \"w\", **profile) as dst:\n",
        "    dst.write(pred_mask.astype(rasterio.uint8), 1)\n",
        "\n",
        "print(f\"Prediction saved at {output_path}\")\n"
      ],
      "metadata": {
        "id": "IrISlG1g1q3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the predicted output\n",
        "output_path = \"/content/predicted_output.tif\"\n",
        "\n",
        "# Read the predicted raster\n",
        "with rasterio.open(output_path) as src:\n",
        "    pred_mask = src.read(1)  # Read the first (and only) band\n",
        "\n",
        "# Plot the predicted mask\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(pred_mask, cmap=\"gray\")  # Use \"gray\" or \"viridis\" for better contrast\n",
        "plt.colorbar(label=\"Class Label\")\n",
        "plt.title(\"Predicted Segmentation Mask\")\n",
        "plt.axis(\"off\")  # Hide axis labels\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eCMeFvB51qz3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}